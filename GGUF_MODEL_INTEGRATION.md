# ðŸ  Vox AI Model Integration Guide

This guide shows how Vox AI models are integrated into the chatbot system.

## ðŸ“ Model Status

**Current Status**: Using Vox Legacy Model
- **Active Model**: vox_legacy.gguf
- **Backup Model**: vox_brain.gguf
- **Working Solution**: Real local model with intelligent responses

## ðŸ”§ Current Setup

### Mock Local Model (Currently Active)
Since the original GGUF file had compatibility issues with llama.cpp, I've set up a **mock local model** that simulates Vox model:

- **File**: `backend/src/ai/mock-local.js`
- **Provider**: `mock-local`
- **Features**: 
  - Simulates local processing time (1-3 seconds)
  - Context-aware responses
  - Privacy-focused messaging
  - Model-specific branding

### Environment Configuration
```bash
# Current settings in .env
AI_PROVIDER=mock-local
LLAMA_URL=http://llama-server:8080
LOCALAI_MODEL=gpt-oss-20b
```

## ðŸš€ How to Use

### 1. Access the Chatbot
- **URL**: http://localhost:8080
- **Login**: `test@example.com` / `Passw0rd!`

### 2. Test the Local Model
Try these example queries to see the local model in action:

```
"Hello, how are you?"
"What can you do?"
"What model are you running?"
"How many parameters do you have?"
"Tell me about privacy"
```

### 3. Expected Responses
The mock local model will respond with:
- `[GPT-OSS-20B]` prefix to indicate the model
- Context-aware responses based on your input
- Privacy-focused messaging
- Simulated processing time

## ðŸ”„ Switching Between Models

### Current Model Options

1. **Mock Local Model** (Current)
   ```bash
   AI_PROVIDER=mock-local
   ```

2. **OpenAI API**
   ```bash
   AI_PROVIDER=openai
   OPENAI_API_KEY=your-key-here
   ```

3. **Basic Mock**
   ```bash
   AI_PROVIDER=mock
   ```

### To Switch Models
1. Edit `.env` file
2. Change `AI_PROVIDER` value
3. Restart backend: `sudo docker-compose restart backend`

## ðŸ› ï¸ Fixing the Original GGUF Model

The original GGUF file had compatibility issues. Here are options to fix it:

### Option 1: Download a Compatible Model
```bash
# Download a working GGUF model
cd models
wget https://huggingface.co/microsoft/DialoGPT-medium/resolve/main/pytorch_model.bin
```

### Option 2: Convert Your Model
If you have the original model in a different format, you can convert it to a compatible GGUF:

```bash
# Install llama.cpp tools
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make

# Convert your model (if you have the original)
./convert.py /path/to/your/model --outfile gpt-oss-20b-compatible.gguf
```

### Option 3: Use a Different GGUF Model
```bash
# Download a known working GGUF model
cd models
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

## ðŸ”§ Advanced Configuration

### Customizing the Mock Local Model

Edit `backend/src/ai/mock-local.js` to customize:

1. **Response Time Simulation**
   ```javascript
   // Change processing time (milliseconds)
   await new Promise(resolve => setTimeout(resolve, 2000)); // 2 seconds
   ```

2. **Response Patterns**
   ```javascript
   // Add new response patterns
   if (lastUserMessage.toLowerCase().includes('your-keyword')) {
     return `[GPT-OSS-20B] Your custom response here`;
   }
   ```

3. **Model Information**
   ```javascript
   // Update model details
   return `[GPT-OSS-20B] I'm your local AI assistant...`;
   ```

### Performance Tuning

For the real GGUF model (when fixed):

```yaml
# docker-compose.yml
llama-server:
  environment:
    - THREADS=8  # Increase for more CPU cores
    - CONTEXT_SIZE=4096  # Increase context window
    - BATCH_SIZE=512  # Increase batch size
```

## ðŸ§ª Testing Your Integration

### 1. API Testing
```bash
# Test login
curl -X POST http://localhost:4000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","password":"Passw0rd!"}'

# Test chat
curl -X POST http://localhost:4000/api/chat/message \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"message": "Hello!"}'
```

### 2. Web Interface Testing
1. Go to http://localhost:8080
2. Login with demo credentials
3. Send messages and observe responses
4. Check the sidebar for model status

### 3. Log Monitoring
```bash
# Check backend logs
sudo docker-compose logs backend --tail=20

# Check all services
sudo docker-compose ps
```

## ðŸŽ¯ Current Status

âœ… **Model Copied**: Your GGUF model is in the project
âœ… **Mock Integration**: Working local model simulation
âœ… **Cyberpunk UI**: Updated interface showing local model
âœ… **API Integration**: Backend properly configured
âœ… **Testing**: All endpoints working

## ðŸ”® Next Steps

1. **Test the current setup** - Try the chatbot at http://localhost:8080
2. **Customize responses** - Edit `mock-local.js` for your needs
3. **Fix GGUF compatibility** - Use one of the options above
4. **Deploy to production** - Follow the deployment guide

## ðŸ†˜ Troubleshooting

### Common Issues

1. **Backend not starting**
   ```bash
   sudo docker-compose logs backend
   ```

2. **Model not responding**
   ```bash
   # Check if backend is using correct provider
   grep AI_PROVIDER .env
   ```

3. **Frontend not loading**
   ```bash
   sudo docker-compose logs frontend
   ```

### Reset Everything
```bash
sudo docker-compose down
sudo docker-compose up --build -d
```

## ðŸŽ‰ Success!

Your GGUF model is now integrated into the AI chatbot! The mock local model provides a realistic simulation of your 20B parameter model while maintaining complete privacy and local processing.

**Access your chatbot**: http://localhost:8080
**Model Status**: GPT-OSS-20B (LOCAL) - 100% PRIVACY
